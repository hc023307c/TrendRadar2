name: Get Hot News (LINE)

on:
  schedule:
    - cron: "33 * * * *" # æ¯å°æ™‚ä¸€æ¬¡ï¼ˆUTCï¼‰
  workflow_dispatch:

concurrency:
  group: crawler-${{ github.ref_name }}
  cancel-in-progress: true

permissions:
  contents: read
  actions: write

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
          clean: true

      - name: Check Expiration
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          WORKFLOW_FILE="crawler.yml"
          API_URL="repos/${{ github.repository }}/actions/workflows/$WORKFLOW_FILE/runs"

          TOTAL=$(gh api "$API_URL" --jq '.total_count')
          if [ -z "$TOTAL" ] || [ "$TOTAL" -eq 0 ]; then
            echo "No previous runs found, skipping expiration check"
            exit 0
          fi

          LAST_PAGE=$(( (TOTAL + 99) / 100 ))
          FIRST_RUN_DATE=$(gh api "$API_URL?per_page=100&page=$LAST_PAGE" --jq '.workflow_runs[-1].created_at')

          if [ -n "$FIRST_RUN_DATE" ]; then
            CURRENT_TIMESTAMP=$(date +%s)
            FIRST_RUN_TIMESTAMP=$(date -d "$FIRST_RUN_DATE" +%s)
            DIFF_SECONDS=$((CURRENT_TIMESTAMP - FIRST_RUN_TIMESTAMP))
            LIMIT_SECONDS=604800

            if [ $DIFF_SECONDS -gt $LIMIT_SECONDS ]; then
              echo "âš ï¸ è¯•ç”¨æœŸå·²ç»“æŸï¼Œè¯·è¿è¡Œ 'Check In' ç­¾åˆ°ç»­æœŸ"
              echo "âš ï¸ Trial expired. Run 'Check In' to renew."
              gh workflow disable "$WORKFLOW_FILE"
              exit 1
            else
              DAYS_LEFT=$(( (LIMIT_SECONDS - DIFF_SECONDS) / 86400 ))
              echo "âœ… è¯•ç”¨æœŸå‰©ä½™ ${DAYS_LEFT} å¤©ï¼Œåˆ°æœŸå‰è¯·è¿è¡Œ 'Check In' ç­¾åˆ°ç»­æœŸ"
              echo "âœ… Trial: ${DAYS_LEFT} days left. Run 'Check In' before expiry to renew."
            fi
          fi

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"
          cache: "pip"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Verify required files
        run: |
          if [ ! -f config/config.yaml ]; then
            echo "Error: Config missing"
            exit 1
          fi

      # 1) è·‘ TrendRadarï¼ŒæŠŠ stdout å­˜èµ·ä¾†ï¼ˆä¸ç®¡æˆåŠŸå¤±æ•—éƒ½ç¹¼çºŒï¼‰
      - name: Run crawler (capture output)
        id: run_crawler
        env:
          # ä½ åŸæœ¬é‚£ä¸€å¤§ä¸² secrets å¯ä»¥ç•™è‘—ï¼ˆé€™è£¡çœç•¥ï¼Œåæ­£ä¸å½±éŸ¿ï¼‰
          GITHUB_ACTIONS: true
        run: |
          set +e
          rm -f trend_stdout.txt trend_message.txt
          python -m trendradar 2>&1 | tee trend_stdout.txt
          echo "exit_code=$?" >> $GITHUB_OUTPUT

      # 2) ç”Ÿæˆ LINE å¥½è®€æ‘˜è¦ï¼ˆè·¯ç·šAï¼šæœ€ç©©ï¼‰
      - name: Build LINE message (clean summary)
        if: always()
        run: |
          set -e
          python - <<'PY'
          import os, re, html
          from datetime import datetime

          def read_text(path):
            try:
              with open(path, "r", encoding="utf-8", errors="ignore") as f:
                return f.read()
            except FileNotFoundError:
              return ""

          # å„ªå…ˆç”¨ output/index.htmlï¼ˆä½ ç›®å‰å·²ç¢ºèªæœƒç”¢å‡ºå®ƒï¼‰
          html_path = "output/index.html"
          src_html = read_text(html_path)

          # æŠ“é€£çµï¼š<a href="...">title</a>
          # ç›¡é‡æŠŠæ¨™é¡Œ->URLå°èµ·ä¾†ï¼ˆå¦‚æœ HTML çµæ§‹ä¸åŒï¼Œä¹Ÿè‡³å°‘ä¸æœƒå£ï¼‰
          link_map = {}
          if src_html:
            for m in re.finditer(r'<a[^>]+href=["\']([^"\']+)["\'][^>]*>(.*?)</a>', src_html, re.I|re.S):
              url = m.group(1).strip()
              title = re.sub(r"<.*?>", "", m.group(2))
              title = html.unescape(title).strip()
              title = re.sub(r"\s+", " ", title)
              if title and url and title not in link_map:
                link_map[title] = url

          # æŠŠ HTML è½‰æˆã€Œå¯åˆ†æçš„ç´”æ–‡å­—ã€
          # 1) å…ˆæŠŠ tag æ‹¿æ‰
          text = src_html
          text = re.sub(r"(?is)<script.*?>.*?</script>", "\n", text)
          text = re.sub(r"(?is)<style.*?>.*?</style>", "\n", text)
          text = re.sub(r"(?is)<br\s*/?>", "\n", text)
          text = re.sub(r"(?is)</p>|</div>|</li>|</tr>|</h\d>", "\n", text)
          text = re.sub(r"(?is)<.*?>", " ", text)
          text = html.unescape(text)
          text = re.sub(r"\r\n?", "\n", text)
          lines = [re.sub(r"\s+", " ", ln).strip() for ln in text.split("\n")]
          lines = [ln for ln in lines if ln]

          # æ¸…æ‰ä½ ç¾åœ¨ LINE çœ‹åˆ°çš„é‚£äº› UI åƒåœ¾å­—
          junk = {
            "ä¿å­˜ä¸ºå›¾ç‰‡","åˆ†æ®µä¿å­˜","çƒ­ç‚¹æ–°é—»åˆ†æ","æŠ¥å‘Šç±»å‹","å½“å‰æ¦œå•","æ–°é—»æ€»æ•°","çƒ­ç‚¹æ–°é—»","ç”Ÿæˆæ—¶é—´",
            "æœ¬æ¬¡æ–°å¢çƒ­ç‚¹","æ¡","æ¥æº: output/index.html","çƒ­ç‚¹æ›´æ–°","TrendRadar çƒ­ç‚¹æ›´æ–°",
          }
          def is_junk_line(ln: str) -> bool:
            if ln in junk: return True
            if re.fullmatch(r"\d+", ln): return True   # å–®ç¨ä¸€å€‹æ•¸å­—ï¼ˆä½ æœ€è¨å­çš„ 1 / 8 / 2 / 11 é‚£ç¨®ï¼‰
            if re.fullmatch(r"\d+\s*æ¡", ln): return True
            return False

          lines2 = [ln for ln in lines if not is_junk_line(ln)]

          # è§£æã€Œå¹³å°æ®µè½ã€ï¼šåƒ â€œä»Šæ—¥å¤´æ¡ Â· 7æ¡â€
          platform_pat = re.compile(r"^(.+?)\s*[Â·â€¢]\s*(\d+)\s*æ¡$")
          platforms = []
          current = None
          for ln in lines2:
            m = platform_pat.match(ln)
            if m:
              if current and current["items"]:
                platforms.append(current)
              current = {"name": m.group(1).strip(), "items": []}
              continue
            if current:
              # é¿å…æŠŠå¤§æ®µã€Œè¯´æ˜æ–‡å­—ã€ç•¶ item
              if len(ln) >= 6 and not ln.startswith(("http://","https://")):
                current["items"].append(ln)

          if current and current["items"]:
            platforms.append(current)

          # åšæˆ LINE å‹å–„æ ¼å¼ï¼šæ¯å¹³å°å–å‰ 5 å‰‡ï¼ˆä½ è¦æ›´å¤šå†èª¿ï¼‰
          now = datetime.now().strftime("%m-%d %H:%M")
          out = []
          out.append(f"ğŸ“° TrendRadar çƒ­ç‚¹æ›´æ–° ({now})")
          out.append("â€”" * 22)

          if not platforms:
            # å¦‚æœè§£æä¸åˆ°ï¼Œå°±é€€å› stdout æœ€å¾Œ 80 è¡Œï¼ˆè‡³å°‘ä¸æœƒç©ºï¼‰
            stdout = read_text("trend_stdout.txt")
            tail = "\n".join(stdout.splitlines()[-80:]).strip()
            out.append("âš ï¸ æœªèƒ½ä» output/index.html è§£æå‡ºæ¦œå•ç»“æ„ï¼Œä»¥ä¸‹ä¸ºè¿è¡Œæ—¥å¿—å°¾æ®µï¼š")
            out.append(tail or "TrendRadar ran, but produced no output.")
          else:
            for p in platforms[:8]:  # æœ€å¤š 8 å€‹å¹³å°ï¼Œé¿å…çˆ†é•·
              out.append(f"\n")
              items = []
              for t in p["items"]:
                # å»æ‰é‡è¤‡ç©ºç™½/å¥‡æ€ªç¬¦è™Ÿ
                t2 = re.sub(r"\s+", " ", t).strip(" -â€”â€¢Â·")
                if not t2:
                  continue
                # é™„ä¸Šé€£çµï¼ˆå¦‚æœæ‰¾åˆ°ï¼‰
                url = link_map.get(t2)
                if url:
                  items.append(f"â€¢ {t2}\n  {url}")
                else:
                  items.append(f"â€¢ {t2}")
                if len(items) >= 5:
                  break
              out.extend(items)

            out.append("\nâ€”" * 22)
            out.append("ï¼ˆæ¯å¹³å°æœ€å¤šæ˜¾ç¤º 5 æ¡ï¼›è¦åŠ é•¿æˆ‘å†å¸®ä½ è°ƒã€‚ï¼‰")

          msg = "\n".join(out).strip()

          # LINE ä¿å®ˆæˆªæ–·ï¼ˆé¿å…éé•·è¢«æ‹’ï¼‰
          msg = msg[:3500]

          with open("trend_message.txt", "w", encoding="utf-8") as f:
            f.write(msg + "\n")

          print("Built trend_message.txt, chars:", len(msg))
          PY

      # 3) æ¨åˆ° LINE
      - name: Push result to LINE
        if: always()
        env:
          LINE_CHANNEL_ACCESS_TOKEN: ${{ secrets.LINE_CHANNEL_ACCESS_TOKEN }}
          LINE_USER_ID: ${{ secrets.LINE_USER_ID }}
        run: |
          set -e
          python - <<'PY'
          import os, json, urllib.request

          token = (os.environ.get("LINE_CHANNEL_ACCESS_TOKEN") or "").strip()
          to = (os.environ.get("LINE_USER_ID") or "").strip()
          if not token or not to:
            raise SystemExit("Missing LINE secrets: LINE_CHANNEL_ACCESS_TOKEN or LINE_USER_ID")

          with open("trend_message.txt", "r", encoding="utf-8", errors="ignore") as f:
            text = (f.read() or "").strip()

          if not text:
            text = "TrendRadar ran, but produced no output."

          payload = {"to": to, "messages": [{"type": "text", "text": text}]}

          req = urllib.request.Request(
            "https://api.line.me/v2/bot/message/push",
            data=json.dumps(payload, ensure_ascii=False).encode("utf-8"),
            headers={
              "Content-Type": "application/json",
              "Authorization": f"Bearer {token}",
            },
            method="POST",
          )

          with urllib.request.urlopen(req, timeout=20) as resp:
            body = resp.read().decode("utf-8", "ignore")
            print("LINE response:", resp.status, body)
          PY
